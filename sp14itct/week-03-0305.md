# week 03/05

- 他很 fundamental, 所只要任何東西是用 random process 之類來描述的, 基本上都可以 apply, 只是不同的地方解釋可能會不一樣

    * 之前我們都是講一個 communication system, 有 input, 有 output, 有 channel, 不過有的地方說一個系統有 memory, 有時候說是 memoryless 的, 他是什麼意思? 就是他不只是 t 的函數(????)

    * 回憶以前學過得 $``$state space$"$, 為什麼以前要學 FSM? 我們做的東西在時間軸與量上都是離散的(連續的需要 quantization, 很麻煩) 如果我們的 communication 不只是 input output 之間的關係, 還有 state, 要怎麼來描述?

    * 我們這邊可以討論的範圍就慢慢擴展, 不只是 input output, 還可以把 state 帶進來, 甚至慢慢去探討他們之間的 dependency. 回憶 Markov: 他跟過去的 $n$ 個有關. Markov chain 跟 Markov process 差別在 Markov chain 跟前 $n$ 個是鍊結的關係, 是比較簡化的模型 (但很有用????)

- Data processing inequality

    * 「若隨機變數間有『上下游關係』, 那無論如何操作都無法增加他們的(某種)資訊量

    * 越接近的 random variable 的 mutual information 會越大.

    * $\displaystyle X\to Y\to Z \Rightarrow I(X;Y)\geq I(X;Z)$

    * COR: If $X\to Y\to Z$ and $Z=g(Y)$, then $I(X;Y)\geq I(X;g(Y))$ //$g$代表某種操作嗎?

    * 在 Shannon 的 mutual information 的定義之下, 我們如何對 $Y$ 操作都無法增加 $I(X;Y)$ 原本有的互相資訊量

- Fano's inequality

    * $H(X|Y)$ 事實上 bound 住用 $Y$ 去猜 $X$ 的犯錯率(機率)

    * Given $(X,Y)$ with (conditional) distribution $(p(x), p(y|x))$ respectively, 我們希望 bound 住 $\hat{X}\neq X$ where $\hat{X}:=g(Y)$ for some $g$

    * Observation: $X\to Y\to \hat{X}$ 組成 Markov chain

    * (Fano's inequality) $\displaystyle H(P_e) + P_e\log(|X|-1)\geq H(X|Y)$, or $\displaystyle P_e\geq \frac{H(X|Y)-1}{\log|X|}$

        + 因為 $P_e$ 是 binary 的, 因此 $H(P_e)\leq 1$ (啥 $p\log p+(1-p)\log (1-p)$ 的)

        + $\log(|X|-1)\leq\log|X|$

    * 另一種用途: 我現在有個source with an unknown variable $X$, Fano告訴我們猜錯的機率下界

- Relative Entropy的性質

    * ...某些跟 difference equation 有關的`QQ` 聽說很有用, 只不過我們平常解的是 deterministic 的

    * $r(x_{n+1}|x_n)$ 比較像是 state transition 的東西; Markov chain 的 state space.

    * $\displaystyle D(p(x_n)\,||\,q(x_n))\geq D(p(x_{n+1})\,||\,q(x_{n+1}))$

    * &Rightarrow; 對任何 Markov chain, p.m.f 之間的距離會隨時間遞減的

    * **(YAY)**

    * Stationary process: 對一個 random process 中的任兩組 random variable $X_p$, $X_q$ 和 $X_i$, $X_j$, 只要 $p-q=i-j$, 那 $(X_p,X_q)$ 和 $(X_i,X_j)$ 的統計特性就完全一樣(mean, variance, ...)

    * 所以 speech 訊號處理的 time frame 都很小, 因為變化太劇烈了

    * Ergodic process: emsemble average = time average. 原本要做非常多次實驗然後取 emsemble average, 現在只要做一次實驗然後用一個區間內的東西平均@@? 還是要用每個period的平均?

- Sufficient statistics

    * WAT?

- Convergence of random variables

    * The sequence $X_1$, $X_2$, ... converges to r.v. $X$

        1. In probability, if $\forall \varepsilon > 0$,

            \\[ \\text{Pr}(|X_n-X|>\\varepsilon) \\to 0 \\]

        1. In mean square, if

            \\[\\text{E}[(X_n-X)^2] \\to 0\\]

        1. With probability 1 (also called $``$almost always $"$), if

            \\[ \\text{Pr}\\left(\\lim_{n\\to\\infty} X_n=\\mu\\right)=1 \\]

- Law of Large Numbers

    * Let $\displaystyle \overline{X_n} := \frac{1}{m}(X_1+X_2+\dots+X_n)$ and assume $\displaystyle \text{E}(\overline{X_n}) \to \mu$ when $n\to\infty$

    * Weak law: converge in sense of (1.)

    * Strong law: converge in sense of (3.)
