# week 02/26 Entropy, Relative Entrpoy, Mutual Information物理量

## Entropy

我有一個 random variable，他們的 entropy 是

\\[ H(x) := -\\sum_x P(x)\\log P(x) \\]

為什麼加入 P = 0 的 random variable(??聽不懂)？有些編碼的技巧能用到

## Joint Entropy and Conditional Entropy

* 為啥要學 joint？因為我們都是個系統，至少兩個，例如 receiver 跟 sender，我們都看他們，所以joint

* conditional probability: 除了描述input，還可以加上條件「他這樣的output是啥」。描述完了input描述完output整個系統就講完了

* 數學上有負的 entropy 嗎？有，連續的case。

    但information theory上基本上都是正的，他是給你information；負的不好理解

* 我們有興趣的：第一個是看它是convex還是concave，所以找及極小/極大

* 本例：無論是 P=0 或 P=1 機率都是 0（給我no information）。

    equal probable 的時候 entropy 最大

* defn of $H(X,Y)$ and $H(Y|X)$

    \\[ H(X,Y)=\\sum_{x,y} -P(x,y)\\log P(x,y) \\]

    \\[\\begin{aligned} H(Y|X) &=^\\text{def} \\sum_x P(x)H(Y|X=x) \\\\
    &= -\\sum_x  P(x) \\sum_y P(y|x)\\log P(y|x) \\\\
    &= -\\sum_{x,y} P(x,y)\\log P(y|x) \\\\
    &= -E_{P(x,y)}\\log P(Y|X)
    \\end{aligned}\\]

* chain rule: $H(X,Y)=H(X)+H(Y|X)$

    joint entropy 是 margin entropy 加上 conditional entropy

* COR: $H(X,Y|Z)=H(X|Z)+H(Y|X,Z)$

* Rem: $H(Y|X)\neq H(X|Y)$：input、output有順序關係

    不像某些系統有duality

    $H(X)-H(X|Y)=H(Y)-H(Y|X)$：$H(X|Y) \Rightarrow$ 知道了某些output後我對input知道的更多事情，這是透過**通訊**產生的。知道了多少？就是 $H(X)-H(X|Y)$。所以跟 $H(Y)-H(Y|X)$ 一樣，也是同一個通訊（$X$：input，$Y$：output）

## Relative entropy

* K-L距離：$\displaystyle D(p || q) = \sum_x p(x) \log \dfrac{p(x)}{q(x)}$ $p$對$q$的距離：實際上是用 $p$（$\log 1/p$：bits）這麼多，但我卻估計了用 $q$ -- 一定有損耗（$\geq 0$）

## Mutual Information（所以牽涉到communication）

* $\displaystyle I(X;Y)=\sum_{x,y} p(x,y)\log \dfrac{p(x,y)}{p(x)p(y)}=D(p(x,y)||p(x)p(y))$

* 探討這個channel有作用/沒作用造成的joint probability的差異

* $X$是channel的input，$Y$是output，那他們應該有某種關係 -- 就是 $p(x,y)$

* 這邊在丈量 $p(x,y)$ 跟 $p(x)p(y)$ 的差距。若channel斷線，那兩者independent，就0了

* Magic happened: $I(X;Y) = \cdots = H(X) - H(X|Y)$

* 所以 $I(X;Y)$ 是 $Y$ 的知識造成的 $X$ 的不確定性減少的量

* $H(X)-H(X|Y)=H(Y)-H(Y|X)$: $X$說的關於$Y$的跟$Y$說的關於$X$的一樣

* also:

    \\[\\begin{aligned} &H(X,Y)=H(X)+H(Y|X), I(X;Y)=H(Y)-H(Y|X) \\\\ &\\Rightarrow I(X;Y)=H(X)+H(Y)-H(X,Y), I(X;X)=H(X) \\end{aligned}\\]

    所以才說 $H$ 是 self information

## Chain rules

* $\displaystyle H(X_1, \dots, X_n) = \sum_i H(X_i|X_{i-1},\dots,X_1)$

    $(...|X_{n-1},\dots,X_1)$: 牽涉到model, 看他們之間的關係怎樣

* $I(X;Y|Z)=H(X|Z)-(X|Y,Z)$

    $\displaystyle I(X_1,\dots,X_N;Y)=\sum_i I(X_i;Y|X_{i-i1},\dots,X_1)$

* $D(p(x,y)||q(x,y))=D(p(x)||q(x))+D(p(y|x)||q(y|x))$

## Convex&Concave: Jensen's Inequality（嗚嗚高微惡夢）

## Information inequality，不等式轟炸

* $I(X;Y)\geq 0$. 等號成立：$p,q$獨立

* $H(X)\leq\log |X|$

* $H(X|Y)\leq H(X) \leq H(X,Y)$

    exercise: do the exercise at page 29

* $\displaystyle H(X_1,X_2,\dots,X_n)\leq \sum_i H(X_i)$. 等號成立：all independent

* (Log sum inequality) $\displaystyle \sum_i a_i\log \dfrac{a_i}{b_i} \geq \left(\sum_i a_i\right)\log\dfrac{\sum_i a_i}{\sum_i b_i}$

    this provides another proof for $D(-||-)\geq 0$

* $\displaystyle D\left(\vphantom{\int}(\lambda p_1+(1-\lambda)p-2)\right|\left|\vphantom{\int}(\lambda q_1+(1-\lambda)q_2)\right) \leq \lambda D(p_1||q_1)+(1-\lambda)D(p_2||q_2)$

* (concavity of entropy) $H(\lambda p_1+(1-\lambda)p_2) \geq \lambda H(p_1)+(1-\lambda)H(p_2)$

* $I(X;Y)$ is:

    + a concave function of $p(x)$ for fixed $p(y|x)$

    + a convex function of $p(y|x)$ for fixed $p(x)$

    1. Fix $p(y|x)$ (output), change $p(x)$: change input = change coding

        It's concave $\Rightarrow$ can get maximum, channel coding，因為有干擾。來源有一定能量的狀況下，傳盡量多的東西過去

    1. Fix $p(x)$, change $p(y|x)$: source coding, minimum: minimum representation

* 下週繼續一堆不等式。一個不等式流傳千古。

    + 我如果不會有沒有關係？還沒關係，除了一個一定要會：(ry
